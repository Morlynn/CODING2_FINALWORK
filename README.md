# CODING2_FINALWORK
## Boids Bubble Machine
My final project is a creative coding project using p5.js and ml5.js, where a simulated ecosystem is controlled through facial recognition technology. This ecosystem includes autonomous agents (boids) and predators that dynamically generate and respond to user facial expressions.

In previous courses, I encountered the concept of a boids system, which is a remarkable tool for simulating ecosystems. Despite being based on simple behavioral principles, boids demonstrate astonishing self-organization and complexity. Through simple behaviors like separation, alignment, and cohesion, individual agents exhibit complex group dynamics reminiscent of behaviors seen in natural phenomena such as bird flocks or fish schools. The mystery of collective behavior in nature gave me a sense of wonder, as if I could create different biological communities through coding, each with its own behaviors and interactions. It felt like I could play the role of a creator.

Later on, I discovered ml5.js, a machine learning library that provided me with various pre-trained models, opening up new possibilities for creative programming projects. I immediately realized that combining ml5.js's face detection API with the boids system could create an interactive ecosystem. By detecting the user's facial expressions, we could integrate users directly into the ecosystem, with their expressions directly influencing the behavior of the boids. This direct human-computer interaction adds vibrancy to the project.

Thus, I decided to merge these two powerful tools. Users can control the evolution of the entire ecosystem through simple facial expressions, observing how their expressions affect the surrounding environment.

In the code section, I used ml5.js's faceApi object for facial recognition, with detectionOptions set to detect facial landmarks. I defined a vector for the nose position (nosePosition) for subsequent simulation. In the draw function, nosePosition and other relevant elements are updated based on the detected facial features. If a face is detected, analyzeFeatures is called to analyze facial features, and based on specific actions (such as closing eyes or opening mouth), boids or predators are added. flock.run(nosePosition) causes all boids to move toward the nose position and handles edge cases to ensure they stay within the canvas. Each predator also updates its position based on the surrounding boids to adjust direction and speed. For facial feature analysis, I used the analyzeFeatures function to examine the state of the eyes and mouth for each detected face and take appropriate actions based on these states. If both eyes are closed, new boids are created at the eye positions. If the mouth is open, new predators are created at the mouth position.

Detecting facial expressions presents a challenge. Detecting open or closed eyes is achieved by comparing the distance between the upper and lower edges of the eyes. If this distance is less than a certain preset threshold, the eyes are considered closed. Mouth detection follows a similar principle. However, this method has limitations. Once the threshold is set, users can only interact within a specific range. If the distance is too far, and the face appears smaller in the frame, even if the eyes are open, they may be detected as closed. This is why, in the video, when the user moves away, boids keep generating from the eye position continuously, almost like a spurt.

The final effect is that when the user closes their eyes, new boids are generated at the closed-eye positions, creating a magical effect as if the user's eyes can "give birth" to new life. When the user opens their mouth, new predators are generated at the mouth position, as if the user can "summon" predators through their mouth to disrupt the ecosystem. Green boids gather and dance around the nose tip, while red boids chase and capture green boids, creating a dynamic and interactive virtual world.
